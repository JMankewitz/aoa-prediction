---
title: Predicting age of acquisition for early words across languages
author: Mika Braginsky, Daniel Yurovsky, Virginia Marchman, and Michael C. Frank
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: tango
    theme: spacelab
---

```{r setup, echo = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE, fig.align = "center")
```

```{r, cache = FALSE, echo = FALSE}
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(ggplot2)
library(langcog)
library(ggrepel)
library(directlabels)
theme_set(theme_mikabr(base_size = 18))
font <- "Open Sans"
```

Get items, words, and uni_lemmas.
```{r}
aoa_data <- feather::read_feather("../aoa_estimation/saved_data/aoa_data.feather")
languages <- unique(aoa_data$language)
norm_lang <- function(lang) {
  lang %>% tolower() %>%
    map_chr(~.x %>% strsplit(" ") %>% unlist() %>% .[1])
}

uni_lemmas <- aoa_data %>%
  select(language, uni_lemma, words) %>%
  distinct()
```

Build up a mapping between CDI items and possible tokens for them in CHILDES.
```{r case_map}
source("stemmer.R")

transforms <- c(
  function(x) gsub("(.*) \\(.*\\)", "\\1", x),
  function(x) gsub(" ", "_", x),
  function(x) gsub(" ", "+", x),
  function(x) gsub("(.+) \\1", "\\1", x)
)

apply_transforms <- function(str) {
  transforms %>% map_chr(~.x(str))
}

special_case_files <- list.files("predictors/childes/special_cases/")

special_case_map <-  map_df(special_case_files, function(case_file) {
  
  lang <- case_file %>% strsplit(".csv") %>% unlist()
  special_cases <- read_csv(file.path("predictors/childes/special_cases/", case_file),
                            col_names = FALSE)

  map_df(1:nrow(special_cases), function(i) {
    uni_lemma <- special_cases$X1[i]
    options <- special_cases[i, 3:ncol(special_cases)] %>%
      as.character() %>%
      discard(is.na)
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique()
    data_frame(language = lang,
               uni_lemma = rep(uni_lemma, 2 * length(trans_opts)),
               stem = c(trans_opts, stem(trans_opts, lang)))
  })
  
})

pattern_map <- uni_lemmas %>%
  split(paste(.$language, .$uni_lemma, .$words)) %>%
  map_df(function(uni_data) {
    language <- uni_data$language %>% norm_lang()
    uni_lemma <- uni_data$uni_lemma
    options <- uni_data$words %>% strsplit(", ") %>% unlist() %>%
      strsplit("/") %>% unlist()
    options <- c(options, stem(options, language)) %>% unique()
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique()
    trans_opts <- c(trans_opts, stem(trans_opts, language)) %>% unique()
    data_frame(language = rep(uni_data$language, length(trans_opts)),
               uni_lemma = rep(uni_lemma, length(trans_opts)),
               stem = trans_opts)
  })

case_map <- bind_rows(special_case_map, pattern_map) %>% distinct()
```

Get measure extracted from CHILDES -- unigram count, mean sentence length, utterance-final position count, singleton count.
```{r load_childes}
load_childes_data <- function(lang) {
  engs <- read_csv(sprintf("predictors/childes/data/childes_%s.csv",
                          norm_lang(lang))) %>%
    filter(!is.na(word)) %>%
    mutate(stem = stem(word, norm_lang(lang)),
           language = lang) %>%
    right_join(case_map %>% filter(language == lang)) %>%
    group_by(uni_lemma) %>%
    summarise(MLU = weighted.mean(mean_sent_length, word_count, na.rm = TRUE),
              word_count = sum(word_count, na.rm = TRUE),
              MLU = ifelse(word_count < 10, NA, MLU),
              final_count = sum(final_count, na.rm = TRUE),
              solo_count = sum(solo_count, na.rm = TRUE),
              language = lang)
}

#safe_log <- function(x) ifelse(x == 0, NaN, log(x))
childes_data <- map_df(languages, load_childes_data)
uni_childes <- childes_data %>%
  filter(word_count != 0) %>%
  group_by(language) %>%
  mutate(word_count = word_count + 1,
         frequency = log(word_count / sum(word_count)),
         final_count = final_count + 1,
         final_freq = log((final_count - solo_count) /
                                     sum(final_count - solo_count)),
         solo_count = solo_count + 1,
         solo_freq = log(solo_count / sum(solo_count)))

uni_childes$final_frequency <- lm(final_freq ~ frequency,
                                  data = uni_childes)$residuals
uni_childes$solo_frequency <- lm(solo_freq ~ frequency,
                                 data = uni_childes)$residuals
```

Get estimates of valence, arousal, and dominance
```{r}
valence <- read_csv("predictors/valence/valence.csv") %>%
  select(Word, V.Mean.Sum, A.Mean.Sum, D.Mean.Sum) %>%
  rename(word = Word, valence = V.Mean.Sum, arousal = A.Mean.Sum,
         dominance = D.Mean.Sum)

#missing_valence <- setdiff(uni_lemmas$uni_lemma, valence$word)
# write_csv(data.frame(uni_lemma = missing_babiness),
#           "predictors/valence/valence_replace.csv")

replacements_valence <- read_csv("predictors/valence/valence_replace.csv")
uni_valences <- uni_lemmas %>%
  left_join(replacements_valence) %>%
  rowwise() %>%
  mutate(word = if (!is.na(replacement) & replacement != "") replacement else uni_lemma) %>%
  select(-replacement) %>%
  left_join(valence) %>%
  select(-word)
  #select(-word, -lexical_class, -lexical_category)

#uni_valences <- read_csv("predictors/valence/uni_lemma_valence.csv")
```

Get estimates of iconicity and babiness.
```{r}
babiness <- read_csv("predictors/babiness_iconicity/english_iconicity.csv") %>%
  group_by(word) %>%
  summarise(iconicity = mean(rating),
            babiness = mean(babyAVG))

#missing_babiness <- setdiff(uni_lemmas$uni_lemma, babiness$word)
# write_csv(data.frame(uni_lemma = missing_babiness),
#           "predictors/babiness_iconicity/babiness_iconicity_replace.csv")

replacements_babiness <- read_csv("predictors/babiness_iconicity/babiness_iconicity_replace.csv")
uni_babiness <- uni_lemmas %>%
  left_join(replacements_babiness) %>%
  rowwise() %>%
  mutate(word = if (!is.na(replacement) & replacement != "") replacement else uni_lemma) %>%
  select(-replacement) %>%
  left_join(babiness) %>%
  select(-word)

#uni_babiness <- read_csv("predictors/babiness_iconicity/babiness_iconicity.csv") 
```

Get estimates of concreteness.
```{r}
concreteness <- read_csv("predictors/concreteness/concreteness.csv")

#missing_concreteness <- setdiff(uni_lemmas$uni_lemma, concreteness$Word)
# write_csv(data.frame(uni_lemma = missing_concreteness),
#           "predictors/concreteness/concreteness_replace.csv")

replacements_concreteness <- read_csv("predictors/concreteness/concreteness_replace.csv")
uni_concreteness <- uni_lemmas %>%
  left_join(replacements_concreteness) %>%
  rowwise() %>%
  mutate(Word = if (!is.na(replacement) & replacement != "") replacement else uni_lemma) %>%
  select(-replacement) %>%
  left_join(concreteness) %>%
  rename(concreteness = Conc.M) %>%
  select(uni_lemma, concreteness)
```

Get (English) phoneme and syllable counts.
```{r}
phonemes <- read_csv("predictors/phonemes/english_phonemes.csv") %>%
  mutate(num_syllables = unlist(map(strsplit(syllables, " "), length)),
         num_phonemes = nchar(gsub("[', ]", "", syllables))) %>%
  select(-phones, -syllables)
```

Put together data and predictors.
```{r}
uni_joined <- aoa_data %>%
  left_join(uni_childes) %>%
  left_join(uni_valences) %>%
  left_join(uni_babiness) %>%
  left_join(uni_concreteness) %>%
  left_join(phonemes) %>%
  distinct()

# all_prop_data <- feather::read_feather("../aoa_estimation/saved_data/all_prop_data.feather")
# uni_props <- all_prop_data %>%
#   select(language, measure, uni_lemma, lexical_classes, words, age, prop) %>%
#   left_join(uni_childes) %>%
#   left_join(uni_valences) %>%
#   left_join(uni_babiness) %>%
#   left_join(uni_concreteness) %>%
#   left_join(phonemes) %>%
#   distinct()
```

Function to get number of characters from item definitions.
```{r}
num_characters <- function(words) {
  words %>%
    strsplit(", ") %>%
    map(function(word_set) {
      word_set %>%
        unlist() %>%
        strsplit(" [(].*[)]") %>%
        unlist() %>%
        strsplit("/") %>%
        unlist() %>%
        gsub("[*' ]", "", .) %>%
        nchar() %>%
        mean()
    }) %>%
    unlist()
}
```

Fit AoA prediction models for each language and measure.
```{r}
predictors <- c("frequency", "MLU", "final_frequency", "solo_frequency",
                "length",
                "concreteness", "valence", "arousal", "babiness")

english <- filter(uni_joined, language == "English")
#ggplot(english, aes(x = log_freq)) + geom_density()
mean_concreteness <- mean(english$concreteness, na.rm = TRUE)
mean_babiness <- mean(english$babiness, na.rm = TRUE)
mean_iconicity <- mean(english$iconicity, na.rm = TRUE)
mean_valence <- mean(english$valence, na.rm = TRUE)
mean_arousal <- mean(english$arousal, na.rm = TRUE)

lang_data_fun <- function(lang, uni_joined, predictors) {
  uni_joined %>%
    filter(language == lang) %>%
    mutate(concreteness = ifelse(is.na(concreteness), mean_concreteness, concreteness),
           babiness = ifelse(is.na(babiness), mean_babiness, babiness),
           valence = ifelse(is.na(valence), mean_valence, valence),
           arousal = ifelse(is.na(arousal), mean_arousal, arousal)) %>%
    rowwise() %>%
    mutate(length = num_characters(words)) %>%
    ungroup() %>%
    select_(.dots = c("language", "measure", "lexical_classes", "uni_lemma", "aoa",
    # select_(.dots = c("language", "measure", "lexical_classes", "uni_lemma", "age", "prop",
                       predictors)) %>%
    group_by(language, measure) %>%
    mutate_each_(funs(as.numeric(scale(.))), predictors) %>%
    ungroup() %>%
    filter(complete.cases(.))
}

lang_model_fun <- function(lang_measure_data, predictors) {
  predictor_formula <- as.formula(
    sprintf("aoa ~ %s", paste(predictors, collapse = " + "))
  )
  lm(predictor_formula, data = lang_measure_data, y = TRUE)
}

# lang_model_fun <- function(lang_measure_data, predictors) {
#   print("Fitting lmer...")
#   predictor_formula <- as.formula(
#     sprintf("prop ~ (1 | uni_lemma) + age + %s", paste(predictors, collapse = " + "))
#   )
#   interaction_formula <- as.formula(
#     sprintf("prop ~ (1 | uni_lemma) + age + %s + %s",
#             paste(predictors, collapse = " + "),
#             paste(sprintf("age * %s", predictors), collapse = " + "))
#   )
#   # m <- glmer(prop ~ age + frequency + (1 | uni_lemma),
#   #            family = "binomial", data = lang_measure_data)
#   no_interact <- glmer(predictor_formula, family = "binomial", data = lang_measure_data)
#   interact <- glmer(interaction_formula, family = "binomial", data = lang_measure_data,
#                     control = glmerControl(optCtrl = list(maxfun = 1e5)))
# }

lang_coef_fun <- function(lang_model) {
  broom::tidy(lang_model) %>%
    filter(term != "(Intercept)") %>%
    select(term, estimate, std.error)
}

all_lang_data <- languages %>%
  map_df(~lang_data_fun(.x, uni_joined, predictors))

# lang_prop_data <- languages %>%
#   map_df(~lang_data_fun(.x, uni_props, predictors))

lang_results <- all_lang_data %>%
  group_by(language, measure) %>%
  nest() %>%
  mutate(model = map(data, ~lang_model_fun(.x, predictors)),
         adj_rsq = map_dbl(model, ~summary(.x)$adj.r.squared),
         coef = map(model, lang_coef_fun))

lang_coefs <- lang_results %>%
  select(language, measure, adj_rsq, coef) %>%
  unnest()
```

Fit AoA prediction models for each measure across languages.
```{r}
crossling_model_fun <- function(measure_data, predictors) {
  print("Fitting lmer...")
  predictor_formula <- as.formula(
    sprintf("aoa ~ %s + (1 + %s | language)",
            paste(predictors, collapse = " + "),
            paste(predictors, collapse = " + "))
  )
  lme4::lmer(predictor_formula, data = measure_data,
             control = lme4::lmerControl(optCtrl = list(maxfun = 1e5)))
}

crossling_coef_fun <- function(crossling_model) {
  data.frame(term = row.names(summary(crossling_model)$coefficients),
             estimate = summary(crossling_model)$coefficients[,"Estimate"],
             std.error = summary(crossling_model)$coefficients[,"Std. Error"],
             row.names = NULL) %>%
  filter(term != "(Intercept)")
}

rsq <- function(object) {
  1 - sum(residuals(object, type = "response") ^ 2) / sum((object$y - mean(object$y)) ^ 2)
}

adj_rsq <- function(object) {
  rsq <- rsq(object)
  p <- summary(object)$df[1] - 1  # p
  n_p <- summary(object)$df[2]  # n - p - 1
  rsq - (1 - rsq) * (p / n_p)
}

crossling_rsq_fun <- function(crossling_model) {
  crossling_fit <- lm(
    model.response(model.frame(crossling_model)) ~ fitted(crossling_model),
    y = TRUE
  )
  adj_rsq(crossling_fit)
}

crossling_models <- all_lang_data %>%
  group_by(measure) %>%
  nest() %>%
  mutate(model = map(data, ~crossling_model_fun(.x, predictors)))

crossling_coefs <- crossling_models %>%
  mutate(coef = map(model, crossling_coef_fun),
         adj_rsq = map_dbl(model, crossling_rsq_fun)) %>%
  select(measure, coef, adj_rsq) %>%
  unnest() %>%
  mutate(language = "All Languages")
```

Combine by-language coefficients with across-language coefficients.
```{r}
term_order <- crossling_coefs %>%
  filter(measure == "understands") %>%
  arrange(desc(abs(estimate)))

joint_coefs <- bind_rows(lang_coefs, crossling_coefs) %>%
  split(.$measure) %>%
  map(~.x %>%
        arrange(desc(adj_rsq)) %>%
        mutate(language = factor(
          language, levels = c("All Languages",
                               unique(language) %>% discard(~.x == "All Languages"))),
               term = factor(term, levels = rev(term_order$term))))
```

```{r coefs, fig.width=12, fig.height=8}
coef_plot <- function(measure) {
  ggplot(joint_coefs[[measure]], aes(x = term, y = estimate)) +
    facet_wrap(~language, ncol = 4) +
    geom_rect(data = filter(joint_coefs[[measure]], language == "All Languages"),
              aes(fill = language), xmin = -Inf, xmax = Inf,
              ymin = -Inf, ymax = Inf, alpha = 0.03) +
    geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                        ymax = estimate + 1.96 * std.error,
                        colour = term)) +
    geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
    geom_text(aes(label = paste("bar(R)^2==", round(adj_rsq, 2))), parse = TRUE,
              y = max(joint_coefs[[measure]]$estimate + 1.96 *
                        joint_coefs[[measure]]$std.error),
              x = 9, family = font, size = 4, hjust = "right") +
    coord_flip() +
    scale_fill_solarized(guide = FALSE) +
    scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
    xlab("") +
    scale_y_continuous(name = "Coefficient Estimate (Months/SD)")
}
coef_plot("understands")
coef_plot("produces")
```

Examine cross-linguistic consistency in coefficient estimates.
```{r consistency, fig.width=9, fig.height=4}
joint_coefs_measures <- bind_rows(joint_coefs) %>%
  mutate(measure = factor(measure, levels = c("understands", "produces")))
ggplot(joint_coefs_measures, aes(x = term, y = estimate)) +
  facet_wrap(~measure) +
  geom_point(aes(colour = term), size = 1, alpha = 0.5,
                      data = joint_coefs_measures %>%
                        filter(language != "All Languages")) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error,
                      colour = term),# size = 1,
                      data = joint_coefs_measures %>%
                        filter(language == "All Languages")) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  coord_flip() +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate (Months/SD)")

coef_var <- joint_coefs_measures %>%
  filter(language != "All Languages") %>%
  group_by(measure, term) %>%
  summarise(crossling_var = var(estimate))

coef_consistency <- joint_coefs_measures %>%
  filter(language == "All Languages") %>%
  left_join(coef_var) %>%
  mutate(var_ratio = crossling_var / abs(estimate)) %>%
  select(measure, term, estimate, std.error, crossling_var, var_ratio)
```

Fit AoA prediction models for each measure and lexical class across languages.
```{r}
lexical_classes <- list("Nouns" = "nouns",
                        "Predicates" = c("adjectives", "adverbs", "verbs"),
                        "Function Words" = "function_words",
                        "Other" = "other")

lexcat_predictors <- predictors %>% discard(~.x %in% c("arousal", "valence"))
crossling_lexcat_models <- all_lang_data %>%
  mutate(mult_lex = map(lexical_classes, ~length(unlist(strsplit(.x, ", "))))) %>%
  filter(mult_lex == 1) %>%
  select(-mult_lex) %>%
  rename(lexical_class = lexical_classes) %>%
  mutate(lexical_class = `levels<-`(factor(lexical_class), lexical_classes)) %>%
  group_by(measure, lexical_class) %>%
  nest() %>%
  mutate(model = map(data, ~crossling_model_fun(.x, lexcat_predictors)))

crossling_lexcat_coefs <- crossling_lexcat_models %>%
  mutate(coef = map(model, crossling_coef_fun),
         adj_rsq = map_dbl(model, crossling_rsq_fun)) %>%
  select(measure, lexical_class, coef, adj_rsq) %>%
  unnest() %>%
  mutate(term = factor(term, levels = rev(term_order$term)))
  # split(.$measure) %>%
  # map(~.x %>%
  #       arrange(desc(adj_rsq)) %>%
  #       mutate(term = factor(term, levels = rev(term_order$term))))
```

```{r lexcat_coefs, fig.width=12, fig.height=6}
plt_lexcat_coefs <- crossling_lexcat_coefs %>%
  bind_rows() %>%
  mutate(measure = factor(measure, levels = c("understands", "produces"))) %>%
  filter(lexical_class != "Other")

pal <- rev(solarized_palette(length(predictors)))
lexcat_pal <- c(pal[1], pal[4:length(pal)])

plt_lexcat_coefs %>%
  ggplot(aes(x = term, y = estimate)) +
    facet_grid(measure ~ lexical_class) +
    geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                        ymax = estimate + 1.96 * std.error,
                        colour = term)) +
    geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
    coord_flip() +
    scale_fill_solarized(guide = FALSE) +
    scale_colour_manual(guide = FALSE,
                        values = lexcat_pal) +
    xlab("") +
    scale_y_continuous(name = "Coefficient Estimate (Months/SD)")

plt_lexcat_coefs %>%
  ggplot(aes(x = term, y = estimate)) +
    facet_grid(measure ~ lexical_class) +
    geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                        ymax = estimate + 1.96 * std.error,
                        colour = term), alpha = 0.1,
                    data = plt_lexcat_coefs) +
    geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                        ymax = estimate + 1.96 * std.error,
                        colour = term),
                    data = plt_lexcat_coefs %>% filter(term %in% c("frequency"))) +
    geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
    coord_flip() +
    scale_fill_solarized(guide = FALSE) +
    scale_colour_manual(guide = FALSE, values = lexcat_pal) +
    xlab("") +
    scale_y_continuous(name = "Coefficient Estimate (Months/SD)")

plt_lexcat_coefs %>%
  ggplot(aes(x = term, y = estimate)) +
    facet_grid(measure ~ lexical_class) +
    geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                        ymax = estimate + 1.96 * std.error,
                        colour = term), alpha = 0.1,
                    data = plt_lexcat_coefs) +
    geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                        ymax = estimate + 1.96 * std.error,
                        colour = term),
                    data = plt_lexcat_coefs %>% filter(term %in% c("MLU"))) +
    geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
    coord_flip() +
    scale_fill_solarized(guide = FALSE) +
    scale_colour_manual(guide = FALSE, values = lexcat_pal) +
    xlab("") +
    scale_y_continuous(name = "Coefficient Estimate (Months/SD)")
```

Compare production vs. comprehension.
```{r compare_measure, fig.width=6, fig.height=5}
plt_crossling_coefs <- crossling_coefs %>%
  mutate(measure = factor(measure, levels = c("understands", "produces")),
         term = factor(term, levels = rev(term_order$term)))

ggplot(plt_crossling_coefs, aes(x = measure, y = estimate, colour = term)) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  geom_linerange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error),
                  position = position_dodge(width = 0.1), alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.1)) +
  geom_line(aes(group = term), position = position_dodge(width = 0.1)) +
  geom_dl(aes(label = term),
          method = list("first.qp", dl.trans(x = x - 1), cex = 0.6,
                        fontfamily = font)) +
  geom_dl(aes(label = term),
          method = list("last.qp", dl.trans(x = x + 1), cex = 0.6,
                        fontfamily = font)) +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate (Months/SD)")

measure_diffs <- crossling_coefs %>%
  select(measure, term, estimate) %>%
  spread(measure, estimate) %>%
  mutate(diff = abs(produces - understands)) %>%
  filter(diff == max(diff))

ggplot(plt_crossling_coefs, aes(x = measure, y = estimate, colour = term)) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  geom_linerange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error),
                  position = position_dodge(width = 0.1), alpha = 0,
                 data = filter(plt_crossling_coefs)) +
  geom_point(position = position_dodge(width = 0.1), alpha = 0,
             data = filter(plt_crossling_coefs)) +
  geom_line(aes(group = term), position = position_dodge(width = 0.1), alpha = 0,
            data = filter(plt_crossling_coefs)) +
  geom_linerange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error),
                  position = position_dodge(width = 0.1), alpha = 0.5,
                 data = filter(plt_crossling_coefs, term %in% measure_diffs$term)) +
  geom_point(position = position_dodge(width = 0.1),
             data = filter(plt_crossling_coefs, term %in% measure_diffs$term)) +
  geom_line(aes(group = term), position = position_dodge(width = 0.1),
            data = filter(plt_crossling_coefs, term %in% measure_diffs$term)) +
  geom_dl(aes(label = term),
          data = filter(plt_crossling_coefs, term %in% measure_diffs$term),
          method = list("first.qp", dl.trans(x = x - 1), cex = 0.6,
                        fontfamily = font)) +
  geom_dl(aes(label = term),
          data = filter(plt_crossling_coefs, term %in% measure_diffs$term),
          method = list("last.qp", dl.trans(x = x + 1), cex = 0.6,
                        fontfamily = font)) +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate (Months/SD)")
```

Fit AoA prediction models for early vs late words across languages.
```{r}
# crossling_age_models <- all_lang_data %>%
#   group_by(measure) %>%
#   mutate(aoa_bin = cut(aoa, breaks = 2, labels = c("earlier", "later"))) %>%
#   group_by(measure, aoa_bin) %>%
#   nest() %>%
#   mutate(model = map(data, ~crossling_model_fun(.x, predictors)))
# 
# crossling_age_coefs <- crossling_age_models %>%
#   mutate(coef = map(model, crossling_coef_fun),
#          adj_rsq = map_dbl(model, crossling_rsq_fun)) %>%
#   select(measure, aoa_bin, coef, adj_rsq) %>%
#   unnest() %>%
#   mutate(language = "All Languages")
```

Compare early vs. late.
```{r compare_age, fig.width=10, fig.height=5}
# plt_crossling_age_coefs <- crossling_age_coefs %>%
#   mutate(measure = factor(measure, levels = c("understands", "produces")),
#          term = factor(term, levels = rev(term_order$term)))
# 
# ggplot(plt_crossling_age_coefs, aes(x = aoa_bin, y = estimate, colour = term)) +
#   facet_wrap(~measure, scales = "free_y") +
#   geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
#   geom_linerange(aes(ymin = estimate - 1.96 * std.error,
#                       ymax = estimate + 1.96 * std.error),
#                   position = position_dodge(width = 0.1), alpha = 0.5) +
#   geom_point(position = position_dodge(width = 0.1)) +
#   geom_line(aes(group = term), position = position_dodge(width = 0.1)) +
#   geom_dl(aes(label = term),
#           method = list("first.qp", dl.trans(x = x - 0.5), cex = 0.6,
#                         fontfamily = font)) +
#   geom_dl(aes(label = term),
#           method = list("last.qp", dl.trans(x = x + 0.5), cex = 0.6,
#                         fontfamily = font)) +
#   scale_fill_solarized(guide = FALSE) +
#   scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
#   scale_y_continuous(name = "Coefficient Estimate (Months/SD)")
# 
# age_diffs <- crossling_age_coefs %>%
#   select(measure, aoa_bin, term, estimate) %>%
#   spread(aoa_bin, estimate) %>%
#   mutate(diff = abs(earlier - later)) %>%
#   group_by(measure) %>%
#   arrange(desc(diff))
# age_diffs_n <- function(n) {
#   age_diffs %>% slice(1:n)
# }
# 
# age_diff_plot <- function(n) {
#   max_age_diffs <- age_diffs_n(n)
#   ggplot(plt_crossling_age_coefs, aes(x = aoa_bin, y = estimate, colour = term)) +
#     facet_wrap(~measure, scales = "free_y") +
#     geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
#     geom_linerange(aes(ymin = estimate - 1.96 * std.error,
#                         ymax = estimate + 1.96 * std.error),
#                     position = position_dodge(width = 0.1), alpha = 0,
#                    data = filter(plt_crossling_age_coefs)) +
#     geom_point(position = position_dodge(width = 0.1), alpha = 0,
#                data = filter(plt_crossling_age_coefs)) +
#     geom_line(aes(group = term), position = position_dodge(width = 0.1), alpha = 0,
#               data = filter(plt_crossling_age_coefs)) +
#     geom_linerange(aes(ymin = estimate - 1.96 * std.error,
#                         ymax = estimate + 1.96 * std.error),
#                     position = position_dodge(width = 0.1), alpha = 0.5,
#                    data = filter(plt_crossling_age_coefs, term %in% max_age_diffs$term)) +
#     geom_point(position = position_dodge(width = 0.1),
#                data = filter(plt_crossling_age_coefs, term %in% max_age_diffs$term)) +
#     geom_line(aes(group = term), position = position_dodge(width = 0.1),
#               data = filter(plt_crossling_age_coefs, term %in% max_age_diffs$term)) +
#     geom_dl(aes(label = term),
#             data = filter(plt_crossling_age_coefs, term %in% max_age_diffs$term),
#             method = list("first.qp", dl.trans(x = x - 0.5), cex = 0.6,
#                           fontfamily = font)) +
#     geom_dl(aes(label = term),
#             data = filter(plt_crossling_age_coefs, term %in% max_age_diffs$term),
#             method = list("last.qp", dl.trans(x = x + 0.5), cex = 0.6,
#                           fontfamily = font)) +
#     scale_fill_solarized(guide = FALSE) +
#     scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
#     xlab("") +
#     scale_y_continuous(name = "Coefficient Estimate (Months/SD)")
# }
# 
# age_diff_plot(1)
```

Examine pairwise correlations between predictors.
```{r}
# predictor_cors <- all_lang_data %>%
#   split(.$language) %>%
#   map_df(function(lang_data) {
#     #lang_data <- crossling_model_data %>% filter(language == "English")
#     expand.grid(predictor1 = crossling_predictor_levels,
#                          predictor2 = crossling_predictor_levels) %>%
#       rowwise() %>%
#       mutate(p = cor.test(lang_data[[as.character(predictor1)]],
#                           lang_data[[as.character(predictor2)]])$p.value,
#              language = unique(lang_data$language))
# #     lang_data %>%
# #       select_(.dots = crossling_predictors) %>%
# #       cor.test() %>%
# #       as.data.frame() %>%
# #       mutate(predictor1 = row.names(.)) %>%
# #       gather(predictor2, cor, -predictor1) %>%
# #       mutate(language = unique(lang_data$language))
#   }) %>%
#   mutate(predictor1 = factor(predictor1, levels = crossling_predictor_levels),
#          predictor2 = factor(predictor2, levels = rev(crossling_predictor_levels))) %>%
#   filter(predictor1 != predictor2)
```

```{r}
# ggplot(predictor_cors, aes(x = predictor1, y = predictor2, fill = abs(cor))) +
#   facet_wrap(~language, ncol = 4) +
#   geom_tile() +
#   geom_text(aes(label = round(cor, 2))) +
#   scale_fill_continuous(low = "white", high = "red") +
#   theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

Make table of words with highest/lowest values for each measure.
```{r}
num_extremes <- 3
extremes <- all_lang_data %>%
  filter(measure == "understands") %>%
    split(.$language) %>%
    map_df(function(lang_data) {
    map_df(predictors, function(predictor) {
      if (predictor %in% c("frequency", "final_frequency", "solo_frequency", "MLU")) {
        filtered_lang_data <- lang_data %>%
          filter(frequency != min(frequency))
      } else {
        filtered_lang_data <- lang_data
      }
      highest <- filtered_lang_data %>%
        arrange_(as.formula(sprintf("~desc(%s)", predictor))) %>%
        .$uni_lemma %>%
        .[1:num_extremes]
      lowest <- filtered_lang_data %>%
        arrange_(predictor) %>%
        .$uni_lemma %>%
        .[1:num_extremes]
      return(data.frame("language" = unique(lang_data$language),
                        "Measure" = predictor,
                        "Highest" = paste(highest, collapse = ", "),
                        "Lowest" = paste(lowest, collapse = ", ")))
    })
  }) %>%
  filter(language == "English") %>%
  select(-language)
```

```{r, cache=FALSE}
knitr::kable(extremes)
```

Make table of instrument sample sizes.
```{r}
sample_sizes <- feather::read_feather("../aoa_estimation/saved_data/sample_sizes.feather")
num_items <- aoa_data %>%
  filter(measure == "understands") %>%
  group_by(language) %>%
  summarise(items = n())
sample_sizes %>%
  spread(form, N) %>%
  left_join(num_items) %>%
  select(language, items, WG, WS) %>%
  rename(Language = language, Items = items) %>%
  knitr::kable()
```

Analysis walk-through plots for English comprehension.
```{r demo_data, fig.width=8, fig.height=4}
demo_lang <- "English"
demo_measure <- "understands"

demo_order <- lang_coefs %>%
  filter(language == demo_lang, measure == demo_measure) %>%
  arrange(desc(abs(estimate)))

demo_data <- all_lang_data %>%
  filter(language == demo_lang, measure == demo_measure) %>%
  gather_("predictor", "value", predictors) %>%
  rename(lexical_class = lexical_classes) %>%
  mutate(predictor = factor(predictor, levels = demo_order$term),
         lexical_class = `levels<-`(factor(lexical_class), lexical_classes))

demo_data %>%
  filter(predictor %in% c("frequency", "MLU")) %>%
ggplot(aes(x = value, y = aoa)) +
  facet_wrap(~predictor, ncol = 5) +
  geom_jitter(#aes(colour = lexical_class),
    colour = solarized_palette(1),
    size = 0.5, alpha = 0.5) +
  geom_smooth(method = "lm", colour = "grey3") +
  scale_colour_solarized(name = "") +
  xlab("Predictor Z-score") +
  scale_y_continuous(name = "Age of Acquisition (months)",
                     breaks = seq(5, 30, 10)) #+
  #scale_alpha(guide = FALSE) +
  #theme(legend.position = "bottom")
```

```{r demo_fits, fig.width=6, fig.height=6}
demo_results <- lang_results %>%
  filter(language == demo_lang, measure == demo_measure)

threshold <- 5
demo_fits <- data_frame(uni_lemma = demo_results$data[[1]]$uni_lemma,
                        lexical_class = demo_results$data[[1]]$lexical_classes,
                        aoa = demo_results$data[[1]]$aoa,
                        predicted = demo_results$model[[1]]$fitted.values) %>%
  mutate(lexical_class = `levels<-`(factor(lexical_class), lexical_classes),
         error = abs(aoa - predicted))

ggplot(demo_fits, aes(x = predicted, y = aoa)) + #, colour = lexical_class)) +
  facet_wrap(~lexical_class, ncol = 2) +
  #coord_fixed() +
  geom_abline(slope = 1, intercept = 0, colour = "grey", linetype = "dashed") +
  geom_point(size = 0.6, colour = solarized_palette(1)) +
  geom_smooth(method = "lm", colour = "black") +
  geom_text_repel(aes(label = uni_lemma), data = filter(demo_fits, error > threshold),
                  size = 4, family = font, force = 27, max.iter = 1e5) +
  #scale_colour_solarized(guide = FALSE) +
  scale_x_continuous(name = "Predicted Age of Acquisition (months)",
                     breaks = seq(5, 25, 5)) +
  scale_y_continuous(name = "Age of Acquisition (months)",
                     breaks = seq(5, 25, 5))
```

```{r demo_coefs, fig.width=8, fig.height=5}
demo_coefs <- lang_coefs %>%
  filter(language == demo_lang, measure == demo_measure) %>%
  #mutate(term = factor(term, levels = rev(demo_order$term)))
  mutate(term = factor(term, levels = rev(term_order$term)))

ggplot(demo_coefs, aes(x = term, y = estimate)) +
  facet_wrap(~language) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error,
                      colour = term)) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  # geom_text(aes(label = paste("bar(R)^2==", round(adj_rsq, 2))), parse = TRUE,
  #           x = 9, y = 1.4, family = font, size = 3) +
  coord_flip() +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate (Months/SD)")
```
