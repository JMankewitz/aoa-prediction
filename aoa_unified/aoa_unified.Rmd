---
title: Predicting age of acquisition for early words across languages
author: "Mika Braginsky, Daniel Yurovsky, Virginia Marchman, Michael Frank"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: tango
    theme: spacelab
---

```{r knitr, echo = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
```

```{r setup, cache = FALSE}
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(ggplot2)
library(langcog)
library(wordbankr)
library(boot)
library(lazyeval)
library(robustbase)
theme_set(theme_mikabr() +
            theme(panel.grid = element_blank(),
                  strip.background = element_blank()))
font <- "Open Sans"
```

Connect to the Wordbank database and pull out the raw data.
```{r wordbank}
data_mode <- "local"

languages <- c("Croatian", "Danish", "English", "French (Quebec)", "Italian",
               "Norwegian", "Russian", "Spanish", "Swedish", "Turkish")

admins <- get_administration_data(mode = data_mode) %>%
  select(language, form, sex, age, data_id) %>%
  filter(language %in% languages)

words <- get_item_data(mode = data_mode) %>%
  filter(type == "word", language %in% languages) %>%
  select(language, form, lexical_class, uni_lemma, definition, item_id) %>%
  mutate(num_item_id = as.numeric(substr(item_id, 6, nchar(item_id))),
         definition = tolower(definition))
```

```{r raw_data, cache.lazy=FALSE}
get_inst_data <- function(inst_items) {
  inst_language <- unique(inst_items$language)
  inst_form <- unique(inst_items$form)
  inst_admins <- filter(admins, language == inst_language, form == inst_form)
  get_instrument_data(instrument_language = inst_language,
                      instrument_form = inst_form,
                      items = inst_items$item_id,
                      administrations = inst_admins,
                      iteminfo = inst_items,
                      mode = data_mode) %>%
    filter(!is.na(age)) %>%
    mutate(produces = !is.na(value) & value == "produces",
           understands = !is.na(value) &
             (value == "understands" | value == "produces")) %>%
    select(-value) %>%
    gather(measure, value, produces, understands) %>%
    mutate(language = inst_language,
           form = inst_form)
}

get_lang_data <- function(lang_items) {
  lang_items %>%
    split(.$form) %>%
    map_df(get_inst_data) %>%
    # production for WS & WG, comprehension for WG only
    filter(measure == "produces" | form == "WG")
}

raw_data <- words %>%
  split(.$language) %>%
  map(get_lang_data)
```

Save point -- raw Wordbank data.
```{r raw_data_save}
#feather::write_feather(bind_rows(raw_data), "saved_data/raw_data.feather")
#raw_data <- feather::read_feather("saved_data/raw_data.feather") %>%
#  split(.$language)
```

```{r sample_sizes}
sample_sizes <- raw_data %>%
  map_dbl(~length(unique(.x$data_id)))

feather::write_feather(
  data_frame(language = names(sample_sizes),
             num_admins = sample_sizes),
  "saved_data/sample_sizes.feather"
)
```

For each language and measure, collapse across age and uni_lemma.
```{r uni_prop_data}
fit_inst_measure <- function(inst_measure_data) {
  
  inst_uni_lemmas <- inst_measure_data %>%
    ungroup() %>%
    select(lexical_class, uni_lemma, definition) %>%
    distinct() %>%
    group_by(uni_lemma) %>%
    summarise(lexical_classes = lexical_class %>% unique() %>% sort() %>%
                paste(collapse = ", "),
              words = definition %>% unique() %>% sort() %>%
                paste(collapse = ", "))
  
  inst_measure_data %>%
    # for each child and uni_lemma, collapse across items
    group_by(language, measure, uni_lemma, age, data_id) %>%
    summarise(uni_value = any(value)) %>%
    # for each age and uni_lemma, collapse across children
    group_by(language, measure, uni_lemma, age) %>%
    summarise(num_true = sum(uni_value, na.rm = TRUE),
              num_false = n() - num_true,
              prop = mean(uni_value, na.rm = TRUE)) %>%
    left_join(inst_uni_lemmas)
  
}

fit_inst <- function(inst_data) {
  
  lang_uni_lemmas <- inst_data %>%
    select(uni_lemma, definition) %>%
    distinct() %>%
    filter(!is.na(uni_lemma))
  
  inst_data_mapped <- inst_data %>%
    select(-uni_lemma) %>%
    left_join(lang_uni_lemmas) %>%
    filter(!is.na(uni_lemma)) %>%
    group_by(definition) %>%
    filter("WG" %in% form)
  
  inst_data_mapped %>%
    split(.$measure) %>%
    map_df(fit_inst_measure)
  
}

uni_prop_data <- map_df(raw_data, fit_inst)
```

Save point -- Wordbank data by collapsed to uni_lemma by age.
```{r uni_prop_data_save}
#feather::write_feather(uni_prop_data, "saved_data/uni_prop_data.feather")
#uni_prop_data <- feather::read_feather("saved_data/uni_prop_data.feather")
```

```{r uni_lemmas}
uni_lemmas <- uni_prop_data %>%
  ungroup() %>%
  select(language, uni_lemma, words) %>%
  distinct()

norm_lang <- function(lang) {
  lang %>% tolower() %>%
    map_chr(~.x %>% strsplit(" ") %>% unlist() %>% .[1])
}
```

Build up a mapping between CDI items and possible tokens for them in CHILDES.
```{r case_map}
source("stemmer.R")

transforms <- c(
  function(x) gsub("(.*) \\(.*\\)", "\\1", x),
  function(x) gsub(" ", "_", x),
  function(x) gsub(" ", "+", x),
  function(x) gsub("(.+) \\1", "\\1", x)
)

apply_transforms <- function(str) {
  transforms %>% map_chr(~.x(str))
}

special_case_files <- list.files("predictors/childes/special_cases/")

special_case_map <-  map_df(special_case_files, function(case_file) {
  
  lang <- case_file %>% strsplit(".csv") %>% unlist()
  special_cases <- read_csv(file.path("predictors/childes/special_cases/", case_file),
                            col_names = FALSE)
  
  map_df(1:nrow(special_cases), function(i) {
    uni_lemma <- special_cases$X1[i]
    options <- special_cases[i, 3:ncol(special_cases)] %>%
      as.character() %>%
      discard(is.na)
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique()
    data_frame(language = lang,
               uni_lemma = rep(uni_lemma, 2 * length(trans_opts)),
               stem = c(trans_opts, stem(trans_opts, lang)))
  })
  
})

pattern_map <- uni_lemmas %>%
  split(paste(.$language, .$uni_lemma, .$words)) %>%
  map_df(function(uni_data) {
    language <- uni_data$language %>% norm_lang()
    uni_lemma <- uni_data$uni_lemma
    options <- uni_data$words %>% strsplit(", ") %>% unlist() %>%
      strsplit("/") %>% unlist()
    options <- c(options, stem(options, language)) %>% unique()
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique()
    trans_opts <- c(trans_opts, stem(trans_opts, language)) %>% unique()
    data_frame(language = rep(uni_data$language, length(trans_opts)),
               uni_lemma = rep(uni_lemma, length(trans_opts)),
               stem = trans_opts)
  })

case_map <- bind_rows(special_case_map, pattern_map) %>% distinct()
```

Get measure extracted from CHILDES -- unigram count, mean sentence length, utterance-final position count, singleton count.
```{r childes}
load_childes_data <- function(lang) {
  read_csv(sprintf("predictors/childes/data/childes_%s.csv",
                   norm_lang(lang))) %>%
    filter(!is.na(word)) %>%
    mutate(stem = stem(word, norm_lang(lang)),
           language = lang) %>%
    right_join(case_map %>% filter(language == lang)) %>%
    group_by(uni_lemma) %>%
    summarise(MLU = weighted.mean(mean_sent_length, word_count, na.rm = TRUE),
              word_count = sum(word_count, na.rm = TRUE),
              MLU = ifelse(word_count < 10, NA, MLU),
              final_count = sum(final_count, na.rm = TRUE),
              solo_count = sum(solo_count, na.rm = TRUE),
              language = lang)
}

childes_data <- map_df(languages, load_childes_data)

uni_childes <- childes_data %>%
  #filter(word_count != 0) %>%
  group_by(language) %>%
  mutate(word_count = word_count + 1,
         frequency = log(word_count / sum(word_count)),
         final_count = final_count + 1,
         final_freq = log((final_count - solo_count) /
                            sum(final_count - solo_count)),
         solo_count = solo_count + 1,
         solo_freq = log(solo_count / sum(solo_count)))

uni_childes$final_frequency <- lm(final_freq ~ frequency,
                                  data = uni_childes)$residuals
uni_childes$solo_frequency <- lm(solo_freq ~ frequency,
                                 data = uni_childes)$residuals
```

Get estimates of valence and arousal.
```{r valence}
valence <- read_csv("predictors/valence/valence.csv") %>%
  select(Word, V.Mean.Sum, A.Mean.Sum) %>%
  rename(word = Word, valence = V.Mean.Sum, arousal = A.Mean.Sum)

replacements_valence <- read_csv("predictors/valence/valence_replace.csv")
uni_valence <- uni_lemmas %>%
  left_join(replacements_valence) %>%
  rowwise() %>%
  mutate(word = if (!is.na(replacement) & replacement != "") replacement else uni_lemma) %>%
  select(-replacement) %>%
  mutate(word = gsub("(.*) \\(.*\\)", "\\1", word)) %>%
  left_join(valence) %>%
  select(-word) %>%
  group_by(language, uni_lemma, words) %>%
  summarise(valence = mean(valence, na.rm = TRUE),
            arousal = mean(arousal, na.rm = TRUE))
```

Get estimates of iconicity and babiness.
```{r babiness}
babiness <- read_csv("predictors/babiness/english_babiness.csv") %>%
  group_by(word) %>%
  summarise(iconicity = mean(rating),
            babiness = mean(babyAVG))

replacements_babiness <- read_csv("predictors/babiness/babiness_replace.csv")

uni_babiness <- uni_lemmas %>%
  left_join(replacements_babiness) %>%
  rowwise() %>%
  mutate(word = if (!is.na(replacement) & replacement != "") replacement else uni_lemma) %>%
  select(-replacement) %>%
  mutate(word = gsub("(.*) \\(.*\\)", "\\1", word)) %>%
  left_join(babiness) %>%
  select(-word)
```

Get estimates of concreteness.
```{r concreteness}
concreteness <- read_csv("predictors/concreteness/concreteness.csv") %>%
  rename(word = Word, concreteness = Conc.M)

replacements_concreteness <- read_csv("predictors/concreteness/concreteness_replace.csv")

uni_concreteness <- uni_lemmas %>%
  left_join(replacements_concreteness) %>%
  rowwise() %>%
  mutate(word = if (!is.na(replacement) & replacement != "") replacement else uni_lemma) %>%
  select(-replacement) %>%
  mutate(word = gsub("(.*) \\(.*\\)", "\\1", word)) %>%
  left_join(concreteness) %>%
  group_by(language, uni_lemma, words) %>%
  summarise(concreteness = mean(concreteness, na.rm = TRUE))
```

Put together data and predictors.
```{r uni_joined}
num_characters <- function(words) {
  words %>%
    strsplit(", ") %>%
    map(function(word_set) {
      word_set %>%
        unlist() %>%
        strsplit(" [(].*[)]") %>%
        unlist() %>%
        strsplit("/") %>%
        unlist() %>%
        gsub("[*' ]", "", .) %>%
        nchar() %>%
        mean()
    }) %>%
    unlist()
}

uni_prop_data_comp <- uni_prop_data %>%
  mutate(length = num_characters(words))

uni_joined <- uni_prop_data_comp %>%
  filter(!grepl("name", uni_lemma)) %>%
  left_join(uni_childes) %>%
  left_join(uni_valence) %>%
  left_join(uni_babiness) %>%
  left_join(uni_concreteness) %>%
  ungroup() %>%
  distinct()
```

Save point -- joined AoA data and predictor data.
```{r uni_joined_save}
#feather::write_feather(uni_joined, "saved_data/uni_joined.feather")
#uni_joined <- feather::read_feather("saved_data/uni_joined.feather")
```

```{r model_setup}
predictors <- c("frequency", "MLU", "final_frequency", "solo_frequency",
                "length", "concreteness", "valence", "arousal", "babiness")

rsq <- function(object) {
  1 - sum(residuals(object, type = "response") ^ 2) /
    sum((object$y - mean(object$y)) ^ 2)
}

adj_rsq <- function(object) {
  rsq <- rsq(object)
  p <- summary(object)$df[1] - 1  # p
  n_p <- summary(object)$df[2]  # n - p - 1
  rsq - (1 - rsq) * (p / n_p)
}

missing <- uni_joined %>%
  select_(.dots = c("language", "uni_lemma", predictors)) %>%
  distinct() %>%
  gather_("predictor", "value", predictors) %>%
  filter(is.na(value)) %>%
  group_by(language, uni_lemma) %>%
  summarise(n = n(),
            missing = paste(predictor, collapse = ", "))

n_missing <- missing %>%
  group_by(language, n) %>%
  summarise(m = n())
```

```{r uni_model_data}
uni_model_data <- uni_joined %>%
  select_(.dots = c("language", "measure", "lexical_classes", "uni_lemma",
                    "age", "num_true", "num_false", predictors)) %>%
  group_by(language, measure) %>%
  mutate_each_(funs(as.numeric(Hmisc::impute(.))), predictors) %>%
  mutate(unscaled_age = age) %>%
  mutate_each_(funs(as.numeric(scale(.))), c(predictors, "age"))

#feather::write_feather(uni_model_data, "saved_data/uni_model_data.feather")
#uni_model_data <- feather::read_feather("saved_data/uni_model_data.feather")
```

```{r lang_models}
lang_model_fun <- function(lang, lang_measure_data) {
  print(sprintf("Fitting lmer for %s...", lang))
  interaction_formula <- as.formula(
    sprintf("cbind(num_true, num_false) ~ (age | uni_lemma) + age + %s",
            paste(sprintf("age * %s", predictors), collapse = " + "))
  )
  lme4::glmer(interaction_formula, family = "binomial", data = lang_measure_data,
              control = lme4::glmerControl(optCtrl = list(maxfun = 1e7)))
}

lang_models <- uni_model_data %>%
  #filter(language %in% c("English", "Russian")) %>%
  group_by(language, measure) %>%
  nest() %>%
  mutate(model = map2(language, data, lang_model_fun))

lexcat_models <- uni_model_data %>%
  filter(!grepl(",", lexical_classes),
         lexical_classes != "other") %>%
  mutate(lexical_category = `levels<-`(
    factor(lexical_classes),
    list(
      "Nouns" = "nouns",
      "Predicates" = c("verbs", "adjectives", "adverbs"),
      "Function Words" = "function_words"
    ))) %>%
  select(-lexical_classes) %>%
  group_by(language, measure, lexical_category) %>%
  nest() %>%
  mutate(model = map2(language, data, lang_model_fun))
```

```{r lang_coefs}
lang_coef_fun <- function(lang_model) {
  broom::tidy(lang_model) %>%
    filter(term != "(Intercept)", term != "age", group == "fixed") %>%
    select(term, estimate, std.error) %>%
    mutate(interaction = ifelse(grepl(":", term), "interaction with age",
                                "main effect"),
           term = gsub("age:", "", term))
}

lang_coefs <- lang_models %>%
  mutate(coefs = map(model, lang_coef_fun)) %>%
  select(language, measure, coefs) %>%
  unnest()

lexcat_coefs <- lexcat_models %>%
  mutate(coefs = map(model, lang_coef_fun)) %>%
  select(language, measure, lexical_category, coefs) %>%
  unnest()

# lang_rsq_fun <- function(lang_model) {
#   lang_fit <- glm(
#     model.response(model.frame(lang_model)) ~ fitted(lang_model),
#     family = "binomial",
#     y = TRUE
#   )
#   adj_rsq(lang_fit)
# }
# 
# lang_rsq <- lang_models %>%
#   mutate(adj_rsq = map_dbl(model, lang_rsq_fun)) %>%
#   select(language, measure, adj_rsq)

# fits <- model.response(model.frame(crossling_model)) %>%
#   as_data_frame() %>%
#   mutate(prop = num_true / (num_true + num_false),
#          fit = fitted(crossling_model))
#
# fits <- lang_models$data[[2]] %>%
#   mutate(prop = num_true / (num_true + num_false),
#          fit = fitted(lang_models$model[[2]]))

# ggplot(fits, aes(x = prop, y = fit)) +
#   geom_point(aes(colour = factor(age)), size = 0.7) +
#   geom_smooth(method = "lm") +
#   #geom_line(aes(group = uni_lemma)) +
#   scale_colour_solarized()
```

```{r lang_base_models}
# lang_base_model_fun <- function(lang, lang_measure_data) {
#   print(sprintf("Fitting lmer for %s...", lang))
#   formula <- as.formula(
#     sprintf("cbind(num_true, num_false) ~ (age | uni_lemma) + age")
#   )
#   lme4::glmer(formula, family = "binomial", data = lang_measure_data)
# }
# 
# lang_base_models <- uni_model_data %>%
#   group_by(language, measure) %>%
#   nest() %>%
#   mutate(model = map2(language, data, lang_base_model_fun))
# 
# lang_base_fits <- lang_base_models %>%
#   group_by(language, measure) %>%
#   mutate(data = map2(data, model, ~.x %>% mutate(fitted = fitted(.y))))
# 
# lang_base_aoas <- lang_base_fits %>%
#   mutate(data = map(
#     data, ~.x %>% group_by(lexical_classes, uni_lemma) %>%
#       summarise(base_aoa = unscaled_age[min(which(fitted > 0.5))])
#   )) %>%
#   select(-model) %>%
#   ungroup() %>%
#   unnest()
# 
# 
# lang_fits <- lang_models %>%
#   group_by(language, measure) %>%
#   mutate(data = map2(data, model, ~.x %>% mutate(fitted = fitted(.y))))
# 
# lang_aoas <- lang_fits %>%
#   mutate(data = map(
#     data, ~.x %>% group_by(lexical_classes, uni_lemma) %>%
#       summarise(aoa = unscaled_age[min(which(fitted > 0.5))])
#   )) %>%
#   select(-model) %>%
#   ungroup() %>%
#   unnest()
# 
# 
# aoas <- lang_aoas %>%
#   left_join(lang_base_aoas)
```

```{r}
# ggplot(aoas, aes(x = base_aoa, y = aoa, colour = lexical_classes)) +
#   facet_wrap(~language, ncol = 5) +
#   coord_equal() +
#   geom_point() +
#   geom_abline(slope = 1, intercept = 0, colour = "grey", linetype = "dashed") +
#   scale_colour_solarized()
```

```{r crossling_models}
# crossling_model_fun <- function(meas, measure_data) {
#   print(sprintf("Fitting lmer for %s...", meas))
#   no_interaction_formula <- as.formula(
#     sprintf("cbind(num_true, num_false) ~
#             (1 | uni_lemma) + (1 | language) + age + %s",
#             paste(predictors, collapse = " + "))
#   )
# #   interaction_formula <- as.formula(
# #     sprintf("cbind(num_true, num_false) ~
# #             (1 | uni_lemma) + (1 | language) + age + %s",
# # #            (age | uni_lemma) + (age | language) + age + %s",
# #             paste(sprintf("age * %s", predictors), collapse = " + "))
# #   )
#   lme4::glmer(no_interaction_formula, family = "binomial", data = measure_data)
#               #control = lme4::glmerControl(optCtrl = list(maxfun = 1e7)))
# }
# 
# 
# eng <- lang_model_fun("English", filter(uni_model_data, measure == "understands",
#                                         language == "English"))
# clm <- lme4::glmer(
#   as.formula("cbind(num_true, num_false) ~
#              (1 | uni_lemma) + (age + frequency | language) + age + frequency + MLU + final_frequency + length + concreteness + valence + arousal + babiness"),
#   # as.formula("cbind(num_true, num_false) ~
#   #            (1 | uni_lemma) + (1 | language) + age + age * frequency + age * MLU + age * final_frequency + age * length + age * concreteness + age * valence + arousal + age * babiness"),
#   family = "binomial",
#   data = filter(uni_model_data, measure == "understands"))
# 
# 
# comp <- crossling_model_fun("understands",
#                             filter(uni_model_data, measure == "understands"))
# prod <- crossling_model_fun("produces",
#                             filter(uni_model_data, measure == "produces"))

# crossling_models <- uni_model_data %>%
#   #filter(language %in% c("English", "Russian")) %>%
#   group_by(measure) %>%
#   nest() %>%
#   mutate(model = map2(measure, data, crossling_model_fun))
# 
# crossling_coefs <- crossling_models %>%
#   mutate(coefs = map(model, lang_coef_fun)) %>%
#   select(measure, coefs) %>%
#   unnest()
```

Combine by-language coefficients with across-language coefficients.
```{r}
term_order <- lang_coefs %>%
  filter(language == "English", measure == "understands",
         interaction == "main effect") %>%
  arrange(desc(abs(estimate))) %>%
  .$term

# term_order <- crossling_coefs %>%
#   filter(measure == "understands") %>%
#   arrange(desc(abs(estimate)))

#joint_coefs <- bind_rows(lang_coefs, crossling_coefs) %>%
# joint_coefs <- lang_coefs %>%
#   split(.$measure) %>%
#   map_df(~.x %>%
#         #arrange(desc(adj_rsq)) %>%
#         mutate(language = factor(
#           language,
#           levels = c("All Languages",
#                      unique(language) %>% discard(~.x == "All Languages"))),
#           term = factor(term, levels = rev(term_order$term)))) %>%

measure_levels <- c("understands", "produces")
effect_levels <- c("main effect", "interaction with age")

lang_coefs <- lang_coefs %>%
  mutate(term = factor(term, levels = rev(term_order)),
         measure = factor(measure, levels = measure_levels),
         interaction = factor(interaction, levels = effect_levels))

lexcat_coefs <- lexcat_coefs %>%
  mutate(term = factor(term, levels = rev(term_order)),
         measure = factor(measure, levels = measure_levels),
         interaction = factor(interaction, levels = effect_levels))

num_coefs <- nrow(term_order)
#feather::write_feather(lang_coefs, "saved_data/lang_coefs.feather")
#feather::write_feather(lexcat_coefs, "saved_data/lexcat_coefs.feather")
```

Examine cross-linguistic consistency in coefficient estimates.
```{r consistency, fig.width=9, fig.height=5}
mean_coefs <- joint_coefs_measures %>%
  group_by(term, measure, interaction) %>%
  summarise(mean = mean(estimate),
            ci_lower = ci_lower(estimate),
            ci_upper = ci_upper(estimate))

ggplot(lang_coefs, aes(x = estimate, y = term)) +
  #facet_grid(measure ~ interaction) + #, scales = "free", space = "free") +
  facet_grid(measure ~ interaction) +
  #facet_grid(. ~ interaction, scales = "free") +
  geom_point(aes(colour = term), size = 1, alpha = 0.4) +
  # data = joint_coefs_measures %>%
  #   filter(language != "All Languages")) +
  geom_point(aes(x = mean, colour = term), size = 3, data = mean_coefs) +
  # geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper, y = mean,
  #                     colour = term),# size = 1,
  #                     data = mean_coefs) +
  geom_vline(xintercept = 0, color = "grey", linetype = "dashed") +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(num_coefs))) +
  ylab("") +
  scale_x_continuous(name = "Coefficient Estimate")

#ggsave("../reports/cogsci_talk/lang_coefs.png", width = 7, height = 5)
```

```{r consistency, fig.width=9, fig.height=5}
plt_lexcat_coefs <- lexcat_coefs %>%
  # mutate(term = factor(term, levels = rev(term_order$term)),
  #        measure = factor(measure, levels = c("understands", "produces")),
  #        interaction = factor(interaction,
  #                             levels = c("main effect", "interaction with age"))) %>%
  filter(interaction == "main effect",
         !(term %in% c("valence", "arousal")))

mean_lexcat_coefs <- plt_lexcat_coefs %>%
  group_by(lexical_category, term, measure, interaction) %>%
  summarise(mean = mean(estimate),
            ci_lower = ci_lower(estimate),
            ci_upper = ci_upper(estimate))

ggplot(plt_lexcat_coefs, aes(x = term, y = estimate)) +
  facet_grid(measure ~ lexical_category) + #, scales = "free", space = "free") +
  #facet_grid(. ~ interaction, scales = "free") +
  geom_point(aes(colour = term), size = 1, alpha = 0.4) +
  # data = joint_coefs_measures %>%
  #   filter(language != "All Languages")) +
  geom_point(aes(y = mean, colour = term), size = 3, data = mean_lexcat_coefs) +
  # geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper, y = mean,
  #                     colour = term),# size = 1,
  #                     data = mean_coefs) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  coord_flip() +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(num_coefs)),
                      drop = FALSE) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate")

#ggsave("../reports/cogsci_talk/lexcat_coefs.png", width = 9, height = 5)
```

Analysis walk-through plots for English comprehension.
```{r demo_data, fig.width=8, fig.height=4}
demo_lang <- "English"
demo_measure <- "understands"

# demo_order <- lang_coefs %>%
#   filter(language == demo_lang, measure == demo_measure) %>%
#   arrange(desc(abs(estimate)))

demo_data <- uni_model_data %>%
  filter(language == demo_lang, measure == demo_measure) %>%
  gather_("predictor", "value", predictors) #%>%
  # rename(lexical_class = lexical_classes) %>%
  # mutate(predictor = factor(predictor, levels = demo_order$term),
  #        lexical_class = `levels<-`(factor(lexical_class), lexical_classes))

demo_data %>%
  filter(predictor %in% c("frequency", "MLU")) %>%
ggplot(aes(x = value, y = aoa)) +
  facet_wrap(~predictor, ncol = 5) +
  geom_jitter(#aes(colour = lexical_class),
    colour = solarized_palette(1),
    size = 0.5, alpha = 0.5) +
  geom_smooth(method = "lm", colour = "grey3") +
  scale_colour_solarized(name = "") +
  xlab("Predictor Z-score") +
  scale_y_continuous(name = "Age of Acquisition (months)",
                     breaks = seq(5, 30, 10)) #+
scale_alpha(guide = FALSE) +
theme(legend.position = "bottom")
```

```{r demo_fits, fig.width=6, fig.height=6}
# demo_results <- lang_results %>%
#   filter(language == demo_lang, measure == demo_measure)
# 
# threshold <- 5
# demo_fits <- data_frame(uni_lemma = demo_results$data[[1]]$uni_lemma,
#                         lexical_class = demo_results$data[[1]]$lexical_classes,
#                         aoa = demo_results$data[[1]]$aoa,
#                         predicted = demo_results$model[[1]]$fitted.values) %>%
#   mutate(lexical_class = `levels<-`(factor(lexical_class), lexical_classes),
#          error = abs(aoa - predicted))
# 
# ggplot(demo_fits, aes(x = predicted, y = aoa)) + #, colour = lexical_class)) +
#   facet_wrap(~lexical_class, ncol = 2) +
#   #coord_fixed() +
#   geom_abline(slope = 1, intercept = 0, colour = "grey", linetype = "dashed") +
#   geom_point(size = 0.6, colour = solarized_palette(1)) +
#   geom_smooth(method = "lm", colour = "black") +
#   geom_text_repel(aes(label = uni_lemma), data = filter(demo_fits, error > threshold),
#                   size = 4, family = font, force = 27, max.iter = 1e5) +
#   #scale_colour_solarized(guide = FALSE) +
#   scale_x_continuous(name = "Predicted Age of Acquisition (months)",
#                      breaks = seq(5, 25, 5)) +
#   scale_y_continuous(name = "Age of Acquisition (months)",
#                      breaks = seq(5, 25, 5))
```

```{r demo_coefs, fig.width=8, fig.height=5}
demo_coefs <- lang_coefs %>%
  filter(language == demo_lang, measure == demo_measure) %>%
  #mutate(term = factor(term, levels = rev(demo_order$term)))
  mutate(term = factor(term, levels = rev(term_order$term)),
         interaction = factor(interaction,
                              levels = c("main effect", "interaction with age")))

ggplot(demo_coefs, aes(x = term, y = estimate)) +
  facet_grid(. ~ interaction) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error,
                      colour = term)) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  # geom_text(aes(label = paste("bar(R)^2==", round(adj_rsq, 2))), parse = TRUE,
  #           x = 9, y = 1.4, family = font, size = 3) +
  coord_flip() +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(length(predictors)))) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate")
```

```{r}
# demo_lang <- "English"
# demo_measure <- "understands"
# 
# demo_data <- uni_model_data %>%
#   filter(language == demo_lang, measure == demo_measure)
# 
# demo_model <- function(formula) {
#   lme4::glmer(formula, family = "binomial", data = demo_data,
#               control = lme4::glmerControl(optCtrl = list(maxfun = 1e7)))
# }
# 
# age_formula <- as.formula(
#   "cbind(num_true, num_false) ~ (1 | uni_lemma) + age"
# )
# 
# age_slope_formula <- as.formula(
#   "cbind(num_true, num_false) ~ (age | uni_lemma) + age"
# )
# 
# predictor_formula <- as.formula(
#   sprintf("cbind(num_true, num_false) ~ (age | uni_lemma) + age + %s",
#           paste(predictors, collapse = " + "))
# )
# 
# interaction_formula <- as.formula(
#   sprintf("cbind(num_true, num_false) ~ (age | uni_lemma) + age + %s",
#           paste(sprintf("age * %s", predictors), collapse = " + "))
# )
# 
# age_model <- demo_model(age_formula)
# age_fit <- fitted(age_model)
# age_slope_model <- demo_model(age_slope_formula)
# age_slope_fit <- fitted(age_slope_model)
# predictor_fit <- fitted(demo_model(predictor_formula))
# interaction_fit <- fitted(demo_model(interaction_formula))
# 
# demo_fits <- demo_data %>%
#   mutate(prop = num_true / (num_true + num_false)) %>%
#   mutate(age_fit = age_fit) %>%
#   mutate(age_slope_fit = age_slope_fit,
#          predictor_fit = predictor_fit,
#          interaction_fit = interaction_fit)
# 
# cor(demo_fits$prop, demo_fits$age_fit)
# cor(demo_fits$prop, demo_fits$age_slope_fit)
# cor(demo_fits$prop, demo_fits$predictor_fit)
# cor(demo_fits$prop, demo_fits$interaction_fit)

# ggplot(fits, aes(x = prop, y = fit)) +
#   geom_point(aes(colour = factor(age)), size = 0.7) +
#   geom_smooth(method = "lm") +
#   #geom_line(aes(group = uni_lemma)) +
#   scale_colour_solarized()
```

```{r}
eng_prod <- uni_model_data %>%
  filter(language == "English", measure == "produces") %>%
  select_(.dots = c("uni_lemma", predictors)) %>%
  distinct()

ggcorplot(eng_prod)
```

Cross-linguistic consistency analyses.
```{r}
empirical_coef_cors <- lang_coefs %>%
  group_by(interaction, measure, term) %>%
  summarise(empirical_coef_var = var(estimate))

baseline_sample <- function(i) {
  lang_coefs %>%
    group_by(interaction, measure, language) %>%
    mutate(term = sample(term)) %>%
    group_by(interaction, measure, term) %>%
    summarise(coef_var = var(estimate)) %>%
    mutate(sample = i)
}

num_samples <- 1000
baseline_samples <- map_df(1:num_samples, baseline_sample)
baseline_summary <- baseline_samples %>%
  group_by(interaction, measure, term) %>%
  summarise(mean_coef_var = mean(coef_var),
            ci_lower_coef_var = ci_lower(coef_var),
            ci_upper_coef_var = ci_upper(coef_var))

coef_cors <- empirical_coef_cors %>%
  left_join(baseline_summary)
```

```{r}
pair_cors <- function(dataset_coefs) {
  by_lang <- dataset_coefs %>% split(.$language)
  map_df(cross2(by_lang, by_lang),
      ~data_frame(language1 = unique(.x[[1]][["language"]]),
                  language2 = unique(.x[[2]][["language"]]),
                  cor = cor(.x[[1]][["estimate"]], .x[[2]][["estimate"]])))
}

coef_cors <- function(coefs) {
  coefs %>%
    group_by(interaction, measure) %>%
    nest() %>%
    mutate(cors = map(data, pair_cors)) %>%
    select(-data) %>%
    unnest()
}

mean_coef_cors <- function(cors) {
  cors %>%
    filter(language1 != language2) %>%
    group_by(interaction, measure) %>%
    summarise(cor = mean(cor))
}

baseline_sample <- function(i) {
  lang_coefs %>%
    group_by(interaction, measure, language) %>%
    mutate(estimate = sample(estimate)) %>%
    coef_cors() %>%
    mean_coef_cors() %>%
    mutate(sample = i)
}

num_samples <- 100
baseline_samples <- map_df(1:num_samples, baseline_sample)
baseline_summary <- baseline_samples %>%
  group_by(interaction, measure) %>%
  summarise(mean_cor = mean(cor),
            ci_lower_cor = ci_lower(cor),
            ci_upper_cor = ci_upper(cor))

empirical_coef_cors <- lang_coefs %>%
  coef_cors()

coef_cor_data <- empirical_coef_cors %>%
  mean_coef_cors() %>%
  left_join(baseline_summary)
```

```{r}
ggplot(coef_cor_data, aes(y = measure)) +
  facet_wrap(~interaction) +
  geom_errorbarh(aes(xmin = ci_lower_cor, xmax = ci_upper_cor, x = mean_cor),
                 height = 0) +
  geom_point(aes(x = mean_cor)) +
  geom_point(aes(x = cor), size = 3) +
  scale_x_continuous(name = "Average pairwise correlation of coefficients",
                     limits = c(-0.1, 1)) +
  ylab("")
```
