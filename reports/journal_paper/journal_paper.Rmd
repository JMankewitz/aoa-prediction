---
title: "From _uh-oh_ to _tomorrow_: Predicting age of acquisition for early words across languages"

# Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
author:
  - name: Mika Braginsky
    affiliation: a,1
  - name: Daniel Yurovsky
    affiliation: b
  - name: Stephan Meylan
    affiliation: c
  - name: Virginia Marchman
    affiliation: d
  - name: Michael C. Frank
    affiliation: d

address:
  - code: a
    address: Department of Brain and Cognitive Sciences, Massachusetts Insitute of Technology, Cambridge, MA 02139
  - code: b
    address: Department of Psychology, University of Chicago, Chicago, IL 60637
  - code: c
    address: Department of Psychology, University of California Berkeley, Berkeley, CA 94720 
  - code: d
    address: Department of Psychology, Stanford University, Stanford, CA 94305

corresponding_author:
  - code: 1
    text: "To whom correspondence should be addressed. Email: mikabr@mit.edu."

author_contributions: |
  Author contributions: TODO

## Remove this if not required
conflict_of_interest: |
  The authors declare no conflict of interest.

abstract: |
  Why do children learn some words earlier than others? Regularities and differences in the acquisition patterns for words across languages yield insights regarding the mechanisms guiding word learning. In a large-scale corpus analysis, we use data from 38,212 children to estimate the developmental trajectories of around 400 words in ten languages, predicting them on the basis of independently-derived linguistic, environmental, and conceptual factors. We examine the consistency and variability of these predictors across languages, by lexical category, and over development. By leveraging data at a significantly larger scale than previous work, our analyses highlight the power that emerges from unifying previously disparate theories, and suggest areas of focus for further theory building.


significance: |
  TODO

acknowledgements: |
  Thanks to the contributors to Wordbank and to NSF BCS \#1528526 for support. 
  
keywords:
  - [TODO keywords]

pnas_type: pnasresearcharticle

bibliography: aoapred.bib
csl: pnas.csl

output: rticles::pnas_article
---


```{r include=FALSE}
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(langcog)
library(wordbankr)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache=TRUE)
theme_set(theme_bw() +
            theme(panel.grid = element_blank(),
                  strip.background = element_blank(),
                  legend.key = element_blank()))
data_path <- "../../aoa_unified/saved_data/"
# uni_joined <- feather::read_feather(
#   "../../aoa_unified/saved_data/uni_joined.feather"
# )
uni_model_data <- feather::read_feather(
  file.path(data_path, "uni_model_data.feather")
)
lang_coefs <- feather::read_feather(
  file.path(data_path, "lang_coefs.feather")
)
lexcat_coefs <- feather::read_feather(
  file.path(data_path, "lexcat_coefs.feather")
)

predictors <- c("frequency", "MLU", "final_frequency", "solo_frequency",
                "length", "concreteness", "valence", "arousal", "babiness")
num_langs <- length(unique(lang_coefs$language))
```

Word learning is one of the central challenges of language acquisition. Learners must integrate multiple information sources to map the word forms they hear onto representations of their meanings. Across many laboratory experiments and small-scale models, a number of strategies have emerged as plausible components of word learning, including tracking co-occurrence statistics between words and referents to deduce word meaning across situations [@yu2007]; attending to social cues like pointing and eye gaze [@hollich2000]; relying on biases, such as a basic level category bias [@markman1990]; drawing on knowledge of relations between words to use known meanings to learn new ones [@markman1988]; and using syntactic structure to constrain interpretations of predicates [@gleitman1990].

Each of these strategies has been reliably demonstrated in the constrained learning context of the laboratory, indicating that they can be used in the word learning process. But small-scale experimental studies typically do not tell us either whether these strategies are used consistently across children, ages, or language communities. It is also difficult to explore how strategies interact to create the longer-term dynamics of vocabulary acquisition. How do the various strategies differ in their relative contributions? And how does their influence change over the course of development?

Our approach to addressing these questions is to use large-scale data on vocabulary development to examine these interactions. By aggregating across a large number of children, we can look past individual differences in acquisition to investigate not only which words are relatively easy or hard to learn, but also what features affect their acquisition.

Previous work using this approach has revealed that in English, within a lexical category, words that are more frequent in speech to children are likely to be learned earlier [@goodman2008]. And further studies have found evidence for semantic networks [@hills2009], neighborhood density [@stokes2010], iconicity [@perry2015], and linguistic distinctiveness [@roy2015] as additional predictors of age of acquisition (AoA), suggesting that they are likely contributors to vocabulary development. But these exciting findings are nevertheless limited in their generality because they used different datasets, focused on different predictors, and almost exclusively analyzed English data. It is thus impossible to compare the relative importance of the many relevant factors under consideration and to draw robust conclusions. 

To remedy this issue, we present analyses based on data from Wordbank ([wordbank.stanford.edu](http://wordbank.stanford.edu)), an open repository of cross-linguistic language development data [@frank2016]. By aggregating administrations of the MacArthur-Bates Communicative Development Inventory [CDI; @fenson2007], a family of parent-report vocabulary checklists, Wordbank provides large-scale vocabulary data based on analogous instruments from more than 60,000 children in more than 20 different language communities. Wordbank presents a novel resource for richer and more powerful analyses of vocabulary learning over development and across languages.

We integrate estimates of words' acquisition trajectories from Wordbank with characterizations of the word learning environment from the CHILDES database [@macwhinney2000] and elsewhere, a data-integration methodology originated by @goodman2008. Building on this work, we examine interactions between a variety of linguistic, environmental, and conceptual factors. Using a similar approach on a high-density longitudinal corpus for a single English-acquiring child, @roy2015 found that the length, usage frequency, and mean length of the utterances in which it occurred were all predictive of a word's AoA. But due to the nature of the dataset, this analysis used production-based AoA estimates and was further limited by relying on data from only one child acquiring a single language.

Our work provides a complimentary analysis by using CDI comprehension and production data available in Wordbank to look at the earliest words that children learn across several different languages. We estimate acqusition trajectories for approximately 400 words from CDIs in each of `r num_langs` languages. We also estimate each word's frequency and mean length of utterance (MLU) based on the set of utterances in CHILDES containing the word. Additionally, we obtain ratings of each word's concreteness, valence, arousal, and relevance to babies from previously collected norms. We use these measures to predict words' acquisition trajectories, assessing the relative contributions of each, as well as how they change over development and interact with lexical category. We use thse analyses to answer two important theoretical questions. 

First, we ask the extent to which there is variability across languages in the relative importance of predictors. For example, are there differences in the importance of grammar-related factors in morphologically more complex languages like Russian and Turkish, compared with simpler ones like English? Differences of this type might be revealing of the degree to which learners face different challenges in different language environments. Alternatively, consistency may suggest the operation of similar learning mechanisms and strategies that are not as dependent on the complexities of phonology, morphology, and syntax in a particular language.

Second, which lexical categories are most influenced by input-related factors, like frequency and utterance length, compared with conceptual factors like concreteness and valence? "Division of dominance" theory suggests that nouns might be more sensitive to cognitive factors, while predicates and closed-class words might be more sensitive to linguistic factors [@gentner2001]. On the other hand, on syntactic bootstrapping theories [@gleitman1990], nouns are argued to be learned via frequent co-occurrence (operationalized by frequency) while verbs might be more sensitive to syntactic factors (operationalized here by utterance length), and neither would be particularly sensitive to conceptual complexity [@snedeker2007]. 

# Approach {#approach .unnumbered}

<!-- We use CDI data from Wordbank to estimate the acquisition trajectories for words across `r num_langs` languages: `r paste(unique(lang_coefs$language), collapse = ", ")`. We then ask what factors are most important for predicting this trajectory.  -->


```{r lang_stats, results="asis"}
# num_admins <- get_administration_data(mode = "local") %>%
#   group_by(language, form) %>%
#   summarise(num_admins = n())
# 
# aoa_stats <- uni_model_data %>%
#   group_by(language, measure) %>%
#   summarise(num_included = n())
# 
# childes_stats <- read_csv("../predictors/frequency/num_words.csv")
# 
# lang_stats <- aoa_stats %>%
#   left_join(num_admins) %>%
#   left_join(childes_stats)
# colnames(lang_stats) <- c("Language", "CDI Items", "CDI Admins", "CHILDES Tokens")
# 
# lang_table <- xtable(lang_stats, digits = 2, caption = "Dataset statistics",
#                      label = "table:lang_stats")
# 
# print(lang_table, type = "latex", comment = FALSE, include.rownames = FALSE,
#       table.placement = "ht", format.args = list(big.mark = ","))
```


To estimate the trajectory words' acquisition, we used vocabulary data collected using CDI instruments, including both Words & Gestures (WG) forms for younger children and  Words & Sentences (WS) forms for older children. When filling out a CDI form, parents are either asked to indicate whether their child "understands" (comprehension) or "understands and says" (production) each of around 400-700 words. Typically, both comprehension and production are queried for younger children and production only is queried for older children, but details vary from adaptation to adaptation. Table \ref{table:lang_stats} gives an overview of our data sources [TODO: port this table].

From these data, for each word on the CDI, we computed the proportion of children at each age who were reported to understand/produce the word. This procedure results in a trajectory reflecting how commonly a word is learned over age (see \ref{fig:example_trajectories} for some examples [TODO: add example trajectories plot]). Then, using logistic mixed effects models, we predict this trajectory on the basis of age, properties of the word, and the interactions between age and each property.


```{r extremes, results='asis'}
num_extremes <- 3
extremes <- uni_model_data %>%
  filter(measure == "understands") %>%
  distinct_(.dots = c("language", "uni_lemma", predictors)) %>%
  split(.$language) %>%
  map_df(function(lang_data) {
    map_df(predictors, function(predictor) {
      if (predictor %in%
          c("frequency", "final_frequency", "solo_frequency", "MLU")) {
        filtered_lang_data <- lang_data %>%
          filter(frequency != min(frequency))
      } else {
        filtered_lang_data <- lang_data
      }
      highest <- filtered_lang_data %>%
        arrange_(as.formula(sprintf("~desc(%s)", predictor))) %>%
        .$uni_lemma %>%
        .[1:num_extremes]
      lowest <- filtered_lang_data %>%
        arrange_(predictor) %>%
        .$uni_lemma %>%
        .[1:num_extremes]
      return(data.frame("language" = unique(lang_data$language),
                        "Predictor" = predictor,
                        "highest" = paste(highest, collapse = ", "),
                        "lowest" = paste(lowest, collapse = ", ")))
    })
  })

extremes_table <- extremes %>%
  filter(language == "English") %>%
  select(-language) %>%
  mutate(order = row_number()) %>%
  gather(Value, Words, highest, lowest) %>%
  arrange(order) %>%
  select(-order) %>%
  mutate(Predictor = list(unique(Predictor), rep("", length(Predictor) / 2)) %>%
           transpose() %>% unlist()) %>%
  xtable::xtable(align = "llll", label = "table:extremes",
                 caption = "Examples of words with the lowest and highest values for each predictor.")

print(extremes_table, type = "latex", comment = FALSE, include.rownames = FALSE,
      table.placement = "b!", floating.environment = "table")
```

```{r lang_coefs, fig.width=7, fig.height=5, fig.env='figure*', fig.align='center', dev='cairo_pdf', fig.cap='TODO'}
num_coefs <- length(unique(lang_coefs$term))

mean_coefs <- lang_coefs %>%
  group_by(term, measure, interaction) %>%
  summarise(mean = mean(estimate),
            ci_lower = ci_lower(estimate),
            ci_upper = ci_upper(estimate))

ggplot(lang_coefs, aes(x = estimate, y = term)) +
  facet_grid(measure ~ interaction) +
  geom_point(aes(colour = term), size = 1, alpha = 0.4) +
  geom_point(aes(x = mean, colour = term), size = 3, data = mean_coefs) +
  geom_vline(xintercept = 0, color = "grey", linetype = "dashed") +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(num_coefs))) +
  ylab("") +
  scale_x_continuous(name = "Coefficient Estimate")
```


```{r lexcat_coefs, fig.width=7, fig.height=4, fig.env='figure*', fig.align='center', dev='cairo_pdf', fig.cap='TODO'}
plt_lexcat_coefs <- lexcat_coefs %>%
  filter(interaction == "main effect",
         !(term %in% c("valence", "arousal")))

mean_lexcat_coefs <- plt_lexcat_coefs %>%
  group_by(lexical_category, term, measure, interaction) %>%
  summarise(mean = mean(estimate),
            ci_lower = ci_lower(estimate),
            ci_upper = ci_upper(estimate))

ggplot(plt_lexcat_coefs, aes(x = term, y = estimate)) +
  facet_grid(measure ~ lexical_category) +
  geom_point(aes(colour = term), size = 1, alpha = 0.4) +
  geom_point(aes(y = mean, colour = term), size = 3, data = mean_lexcat_coefs) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  coord_flip() +
  scale_fill_solarized(guide = FALSE) +
  scale_colour_manual(guide = FALSE, values = rev(solarized_palette(num_coefs)),
                      drop = FALSE) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate")
```

Each of our predictors is derived from independent sources. For each word that appears on the CDI forms in each of our `r num_langs` languages, we obtained an estimate of its frequency in child-directed speech, the mean length of utterances in which it appears in child-directed speech, its length in characters, and ratings of its concreteness, valence, arousal, and relevance to babies. Items such as _child's own name_ were excluded. Example words for these predictors in English are shown in Table \ref{table:extremes}. Frequency and mean length of utterance were measured relative to the word's language. But since existing datasets for conceptual ratings are primarily available for English, we mapped all words onto translation equivalents across CDI forms, allowing us to use the ratings for English words across languages. While necessarily imperfect, this method allows us to examine languages for which limited resources exist. 

# Results {#results .unnumbered}

Figure \ref{fig:lang_coefs} shows the coefficient estimate for each predictor in each language. We find that babiness, frequency, MLU, and concreteness are relatively stronger predictors of age of acquisition across languages. Given the emphasis on frequency effects in the language acquisition literature [@ambridge2015], one might have expected frequency to dominate, but several other predictors are just as strong in this analysis. Some factors previously argued to be important for word learning, namely valence and arousal, appear to have limited relevance when compared to other factors.

A potential concern for comparing these coefficient estimates is predictor collinearity. Fortunately, in every language, the only high correlations were between frequency and number of characters, a reflection of Zipf's Law [@zipf1935], and between frequency and concreteness, probably as a consequence of the complexity bias [@lewisunderreview].

Overall, there is considerable consistency in how the predictors pattern in various languages -- a priori, it could have been the case that different languages have wildly different effects of experiential vs. structural factors, but this is not what we seem to find. Another observation of note is the differences between coefficient estimates between measures: the largest asymmetry is displayed by length, which is far more predictive for production than comprehension. Thus as measured here, length seems to be playing more the role of how difficult a word is to say than how difficult it is to remember.

Next, we wanted to examine not just the contributions of various factors to word learning, but also how their relative importance changes over development. Across languages, positive age interactions can be seen for concreteness and frequency (i.e. their effects increase with age). Conversely, there are negative age interactions for babiness and valence in comprehension and for solo frequency in production.

Previous work gives reason to believe that predictors' relationship with age of acquisition differs among various lexical categories [@goodman2008]. To investigate these effects, we separated our data by lexical category and fit separate models for each category. Figure \ref{fig:lexcat_coefs} shows the resulting coefficient estimates. Frequency matters most for nouns and comparatively little for function words, while MLU is irrelevant for both nouns and predicates, but is highly informative for function words.


# Discussion {.unnumbered}

What makes words easier or harder for young children to learn? Previous experimental work has largely addressed this question using small-scale experiments. While such experiments can identify sources of variation, they typically do not allow for different sources to be compared in detail. In contrast, observational studies allow the effects of individual factors (with frequency being the most common) to be measured across ages and lexical categories [e.g., @goodman2008]. Scale comes at a cost in terms of detail, however, since the availability of both predictors and outcome data has been quite limited. 

By including `r num_langs` languages and `r num_coefs` predictors, our current work expands the scope of previous observational studies of age of acquisition. Our data show a number of patterns that confirm and expand previous reports. First, predictors changed in relative importance across development. For example, certain concepts that were more strongly associated with babies appeared to be learned early for children across languages [as in @tardif2008].

Second, we found general consistency in predictor coefficients across languages (even as overall model fit varied, at least in part due to the amount and quality of data for different languages). This consistency supports the idea that differences in culture or language structure do not lead to fundamentally different acquisition strategies, at least at the level of detail we were able to examine. 

Lastly, the predictors varied in strength across lexical categories. Frequent, concrete nouns were learned earlier, consistent with theories that emphasize the importance of early referential speech [e.g., @baldwin1995]. But for predicates, concreteness was somewhat less important, and for function words, MLU was most predictive. Overall these findings are consistent with theories that emphasize the role of linguistic structure over conceptual complexity in the acquisition of other lexical categories beyond nouns [@gentner2001; @snedeker2007].

Despite its larger scope, our work shares a number of important limitations with previous studies. First and foremost, our approach is to predict one set of individuals with data about the experience of a completely different set and ratings of concepts gathered from yet others. In contrast to dense-data analyses [@roy2015], this approach fundamentally limits the amount of variability we will be able to capture. In addition, the granularity of the predictors that can be extracted from corpus data and applied to every word is necessarily quite coarse. Ideally, predictors could be targeted more specifically at particular theoretical constructs of interest (for example, the patterns of use for specific predicates).

[TODO: end on a high note?]

```{r}
# [TODO: is there a way to make this point?]
#Finally, our work underscores the incompleteness of the current understanding of vocabulary development. Even for English, the language in which our model captures the most variance ($r^2 = 0.29$), much still remains unexplained. Furthermore, this variance is highly reliable---cross-validation using half of the English-speaking children to predict ages of acquisition for the other half yields $r^2 = 0.98$. This gap highlights an important theoretical challenge in the study of early language: linking individual datapoints to the broader patterns of acquisition. We have strong theories of how individual learning situations proceed, but must unify these theories to make progress on understanding language learning at scale.
```

## Materials and Methods

All code and data to reproduce our analyses is available at [https://github.com/mikabr/aoa-prediction](https://github.com/mikabr/aoa-prediction).

### Predictor variables

Each numeric predictor was centered and scaled so that all predictors would have comparable units. 

Translation equivalents are available in the Wordbank database using the \texttt{wordbankr} package in \texttt{R} [@frank2016]. Translation equivalents were constructed by the authors and independently hand-checked by native speakers.

*Lexical category.* Category was determined on the basis of the conceptual categories presented on the CDI form (e.g., "Animals"), such that the Nouns category contains common nouns, Predicates contains verbs and adjectives, Function Words contains closed-class words, and Other contains the remaining items [following @bates1994].

*Frequency.* For each language, we estimated word frequency from unigram counts based on all corpora in CHILDES for that language. Each word's count includes the counts of words that share the same stem (so that "dogs" counts as "dog") or are synonymous (so that "father" counts as "daddy"). For polysemous word pairs (e.g., "orange" as in color or fruit), occurrences of the word in the corpus were split uniformly between the senses on the CDI. Counts were normalized to the length of each corpus and then log transformed.

*Solo and Final Frequencies.* Using the same dataset as for frequency, we estimated the frequency with which each of word occurs as the sole word in an utterance, and the frequency with which it appears as the final word of an utterance (not counting single-word utterances). Since both of these estimates are by necessity highly correlated with frequency, we residualized unigram frequency out of both of them, so that values reflect an estimate of the effect of solo/final frequency over and above frequency itself.

*MLU.* For each language, we estimated each word's MLU by calculating the mean length in words of the utterances in which that word appeared, for all corpora in CHILDES for that language. Words that only occurred in one utterance were excluded.

*Length.* We computed the number of characters in each word in each language. While imperfect, this metric of length is highly correlated with number of phonemes and syllables [@lewis2016].

*Concreteness.* We used previously collected norms for concreteness [@brysbaert2014], which were gathered by asking adult participants to rate how concrete the meaning of each word is on a 5-point scale from abstract to concrete. For the [TODO] CDI words that were not part of the collected norms, we imputed ratings from the mean of all CDI words' ratings.

*Valence and Arousal.* We also used previously collected norms for valence and arousal [@warriner2013], for which adult participants were asked to rate words on a 1-9 happy-unhappy scale (valence) and 1-9 excited-calm scale (arousal). For the TODO CDI words that were not part of the collected norms (mostly function words), we imputed ratings from the mean of all CDI words' ratings.

*Babiness.* Lastly, we used previously collected norms of "babiness", a measure of association with infancy [@perry2015] for which adult participants were asked to judge a word's relevance to babies.

### Analysis

For all analyses, we used logistic mixed-effects regression models (fit with \texttt{lme4 1.1-12} in \texttt{R}) to predict the number of children who understand/produce each word at each age from age, the above predictors, and the interactions of age with the predictors. Each model was fit to all data from a particular language community and included a random intercept for each word and a random slope of age for each word. 

<!-- Leave these lines as they are at the end of your .Rmd file to ensure placement of methods & acknowledgements sections before the references-->
\showmatmethods
\showacknow
\pnasbreak

\newpage